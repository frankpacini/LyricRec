{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5Je1JTm1u2A"
   },
   "outputs": [],
   "source": [
    "# Code to read csv file into Colaboratory:!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "\n",
    "# Load files from google drive data folder using share link\n",
    "files = {\"1f1WgnkjeKy6AouMEWOSwNwN1VynIDZAr\":\"mxm_reverse_mapping.txt\", \n",
    "         \"1GIAGz9-Cajao_fodTbRLtuBIxtp4oEKl\":\"mxm_dataset_train.txt\",\n",
    "         \"1mahH9h_KQK3WZBm6ZCeVKQyqCpYefgyK\":\"mxm_dataset_test.txt\",\n",
    "         \"1GDBkvh7YjigZLO2Pfa00kH0rLmtO2luv\":\"ft_params.pkl\",\n",
    "         \"1Nx31erkTutZe-kqBAAn-hSwUvBHCzshI\":\"ft_postspec.txt\"}\n",
    "for id in files.keys():\n",
    "  downloaded = drive.CreateFile({'id':id}) \n",
    "  downloaded.GetContentFile(files[id])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mM99Bptd4I-V"
   },
   "outputs": [],
   "source": [
    "# Code from models.py to load external embeddings\n",
    "\n",
    "import codecs\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, cuda, stddev=0.2):\n",
    "        super(GaussianNoise, self).__init__()\n",
    "        self.stddev = stddev\n",
    "        self.cuda = cuda\n",
    "\n",
    "    def forward(self, din):\n",
    "        if self.training:\n",
    "            noise = torch.autograd.Variable(torch.randn(din.size()))\n",
    "            noise = noise.cuda() if self.cuda else noise\n",
    "            return din + noise * self.stddev\n",
    "        return din\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.emb_dim = params.emb_dim\n",
    "        self.dis_layers = params.dis_layers\n",
    "        self.dis_hid_dim = params.dis_hid_dim\n",
    "        self.dis_dropout = params.dis_dropout\n",
    "        self.dis_input_dropout = params.dis_input_dropout\n",
    "\n",
    "        layers = [nn.Dropout(self.dis_input_dropout)]\n",
    "        if params.noise:\n",
    "            layers.append(GaussianNoise(params.cuda))    \n",
    "        for i in range(self.dis_layers + 1):\n",
    "            input_dim = self.emb_dim if i == 0 else self.dis_hid_dim\n",
    "            output_dim = 1 if i == self.dis_layers else self.dis_hid_dim\n",
    "            layers.append(nn.Linear(input_dim, output_dim))\n",
    "            if i < self.dis_layers:\n",
    "                layers.append(nn.LeakyReLU(0.2))\n",
    "                layers.append(nn.Dropout(self.dis_dropout))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.dim() == 2 and x.size(1) == self.emb_dim\n",
    "        return self.layers(x).view(-1)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.emb_dim = params.emb_dim\n",
    "        self.dis_layers = params.gen_layers\n",
    "        self.dis_hid_dim = params.gen_hid_dim\n",
    "        self.dis_dropout = params.gen_dropout\n",
    "        self.dis_input_dropout = params.gen_input_dropout\n",
    "\n",
    "        layers = [nn.Dropout(self.dis_input_dropout)]\n",
    "        for i in range(self.dis_layers + 1):\n",
    "            input_dim = self.emb_dim if i == 0 else self.dis_hid_dim\n",
    "            output_dim = self.emb_dim if i == self.dis_layers else self.dis_hid_dim\n",
    "            layers.append(nn.Linear(input_dim, output_dim))\n",
    "            if i < self.dis_layers:\n",
    "                layers.append(nn.LeakyReLU(0.2))\n",
    "                layers.append(nn.Dropout(self.dis_dropout))\n",
    "                if params.noise:\n",
    "                    layers.append(GaussianNoise(params.cuda))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.dim() == 2 and x.size(1) == self.emb_dim\n",
    "        return self.layers(x)\n",
    "\n",
    "def load_external_embeddings(params, emb_path):\n",
    "    \"\"\"\n",
    "    Reload pretrained embeddings from a text file.\n",
    "    \"\"\"\n",
    "    \n",
    "    word2id = {}\n",
    "    vectors = []\n",
    "\n",
    "    # load pretrained embeddings\n",
    "    _emb_dim_file = params.emb_dim\n",
    "    with codecs.open(emb_path) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if len(line.split()) == 2:\n",
    "                i -= 1\n",
    "                continue\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            if np.linalg.norm(vect) == 0:  # avoid to have null embeddings\n",
    "                vect[0] = 0.01\n",
    "            assert word not in word2id\n",
    "            assert vect.shape == (_emb_dim_file,), i\n",
    "            word2id[word] = len(word2id)\n",
    "            vectors.append(vect[None])\n",
    "\n",
    "    logging.info(\"Loaded %i pre-trained word embeddings\" % len(vectors))\n",
    "\n",
    "    # compute new vocabulary / embeddings\n",
    "    \n",
    "    dico = word2id\n",
    "\n",
    "    # id2word mapping is not necessary and avoids defining a Dictionary class\n",
    "    # id2word = {v: k for k, v in word2id.items()}\n",
    "    #dico = id2word.copy()\n",
    "    #dico = dico.update(word2id)\n",
    "    \n",
    "    embeddings = np.concatenate(vectors, 0)\n",
    "    embeddings = torch.from_numpy(embeddings).float()\n",
    "    embeddings = embeddings.cuda() if params.cuda and torch.cuda.is_available() else embeddings\n",
    "    assert embeddings.size() == (len(word2id), params.emb_dim), ((len(word2id), params.emb_dim, embeddings.size()))\n",
    "\n",
    "    return dico, embeddings\n",
    "\n",
    "\n",
    "def normalize_embeddings(emb, types):\n",
    "    \"\"\"\n",
    "    Normalize embeddings by their norms / recenter them.\n",
    "    \"\"\"\n",
    "    for t in types.split(','):\n",
    "        if t == '':\n",
    "            continue\n",
    "        if t == 'center':\n",
    "            emb.sub_(emb.mean(1, keepdim=True).expand_as(emb))\n",
    "        elif t == 'renorm':\n",
    "            emb.div_(emb.norm(2, 1, keepdim=True).expand_as(emb))\n",
    "        else:\n",
    "            raise Exception('Unknown normalization type: \"%s\"' % t)\n",
    " \n",
    "def build_model(params, with_dis):\n",
    "    \"\"\"\n",
    "    Build all components of the model.\n",
    "    \"\"\"\n",
    "    # source embeddings\n",
    "    src_dico, _src_emb = load_external_embeddings(params, params.seen_file)\n",
    "    params.src_dico = src_dico\n",
    "    src_emb = nn.Embedding(len(src_dico), params.emb_dim, sparse=True)\n",
    "    src_emb.weight.data.copy_(_src_emb)\n",
    "\n",
    "    # target embeddings\n",
    "    tgt_dico, _tgt_emb = load_external_embeddings(params, params.adjusted_file)\n",
    "    params.tgt_dico = tgt_dico\n",
    "    tgt_emb = nn.Embedding(len(tgt_dico), params.emb_dim, sparse=True)\n",
    "    tgt_emb.weight.data.copy_(_tgt_emb)\n",
    "\n",
    "    # mapping\n",
    "    mapping = Generator(params)\n",
    "\n",
    "    # discriminator\n",
    "    discriminator = Discriminator(params)\n",
    "\n",
    "    # cuda\n",
    "    if params.cuda:\n",
    "        src_emb.cuda()\n",
    "        tgt_emb.cuda()\n",
    "        mapping.cuda()\n",
    "        discriminator.cuda()\n",
    "\n",
    "    # normalize embeddings\n",
    "    normalize_embeddings(src_emb.weight.data, params.normalize_embeddings)\n",
    "    normalize_embeddings(tgt_emb.weight.data, params.normalize_embeddings)\n",
    "\n",
    "    return src_emb, tgt_emb, mapping, discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_HwEe1h3ljh",
    "outputId": "890e02e2-2512-45c6-ad07-8eb798d6908e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "Reverse map\n",
      "Training set\n",
      "Fast text embeddings\n",
      "Song dict\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords\n",
    "remove_these = set(stopwords.words('english'))\n",
    "\n",
    "REMOVE_STOPWORDS = True\n",
    "\n",
    "def get_song_emb_dict(dataset):\n",
    "\n",
    "    with open('mxm_reverse_mapping.txt','r') as f:        # Load mapping from contracted word to full word string in the mxm dataset\n",
    "        lines = f.readlines()\n",
    "        map = {}\n",
    "        for l in lines:\n",
    "            input, output = l.split(\"<SEP>\")\n",
    "            map[input] = output   \n",
    "\n",
    "    print(\"Reverse map\")\n",
    "\n",
    "    with open(dataset,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        words = lines[17].replace('%','').split(',')      # get list of words which will be referenced by index in the dataset\n",
    "        songs_dict = {}\n",
    "\n",
    "        for i,l in list(enumerate(lines))[18:]:     # a line represents data on a song, with the first comma separated value being the MSDID, and the remainder being a word index followed by its count\n",
    "            song_info = l.split(',')\n",
    "            MSDID = song_info[0]\n",
    "            song_bow = [x.split(':') for x in song_info[2:]]\n",
    "            song_dict = {}\n",
    "            for word, word_count in song_bow:\n",
    "                song_dict[int(word)] = int(word_count.replace('\\n',''))\n",
    "\n",
    "            # word_lists = [[words[word-1]]*song_dict[word] for word in song_dict.keys()]\n",
    "\n",
    "            song = [  (map[words[word-1].replace('\\n','')].replace('\\n',''),  song_dict[word]) \n",
    "                    for word in song_dict.keys()]\n",
    "            if REMOVE_STOPWORDS:\n",
    "                # song = [(map[w[0].replace('\\n','')].replace('\\n',''),w[1]) for w in song if w[0] not in remove_these]\n",
    "                song = [s for s in song if s[0] not in remove_these]      # Filter out words considered frequent words in the ntlk stopwords dataset\n",
    "            \n",
    "            songs_dict[str(MSDID)] = song     # songs_dict: MSDID -> (word, count) list\n",
    "\n",
    "    print(\"Training set\")\n",
    "\n",
    "    song_msd_ids = list(songs_dict.keys())\n",
    "    # print(all_songs_dict[song_msd_ids[2]])\n",
    "\n",
    "    pkl_file = open(\"ft_params.pkl\", 'rb')\n",
    "    params = pickle.load(pkl_file)\n",
    "    # mapping = Generator(params)\n",
    "    # checkpoint = torch.load(\"./models/ft_model.t7\")\n",
    "    # mapping.load_state_dict(checkpoint['model'])\n",
    "    # mapping.eval()\n",
    "    out_dico, out_emb = load_external_embeddings(params, \"ft_postspec.txt\")         # out_dico: word -> idx, out_emb: idx -> vector\n",
    "\n",
    "    print(\"Fast text embeddings\")\n",
    "\n",
    "    all_song_emb_dict = {} # {id: [(out_emb[out_dico[\"en_\" + tup[0]]], tup[1]) for tup in songs_dict[id] if \"en_\" + tup[0] in out_dico] for id in song_msd_ids}\n",
    "\n",
    "    # Generate set of all words using dictionary (mapping to global word frequency)\n",
    "    word_set = {}                     \n",
    "    for id in song_msd_ids:\n",
    "        for tup in songs_dict[id]:\n",
    "            if tup[0] in word_set:\n",
    "                word_set[tup[0]] = word_set[tup[0]] + tup[1]\n",
    "            else:\n",
    "                word_set[tup[0]] = tup[1]\n",
    "\n",
    "    # Find the most frequent words not in the AuxGAN set\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        mapped_emb = mapping(out_emb).data.cpu().numpy()\n",
    "\n",
    "    missed_words = {}\n",
    "    for id in song_msd_ids:\n",
    "        for tup in songs_dict[id]:\n",
    "            if \"en_\" + tup[0] not in out_dico:\n",
    "                if tup[0] not in missed_words:\n",
    "                    missed_words[tup[0]] = tup[1]\n",
    "                else:\n",
    "                    missed_words[tup[0]] = missed_words[tup[0]] + tup[1]\n",
    "    pp.pprint(sorted(missed_words.items(), key=lambda x: x[1], reverse=True))\n",
    "    \"\"\"\n",
    "  \n",
    "    filtered_words = [word for word in word_set.keys() if (\"en_\" + word) in out_dico]             # Filter out words not in the AuxGAN set\n",
    "    # emb_list = [(out_emb[out_dico[\"en_\" + word]], word_set[word]) for word in filtered_words]\n",
    "    emb_dict = {word: out_emb[out_dico[\"en_\" + word]] for word in filtered_words}\n",
    "\n",
    "    return songs_dict, song_msd_ids, emb_dict\n",
    "\n",
    "# def get_all_word_emb():\n",
    "\n",
    "songs_dict, ids, word_emb_dict = get_song_emb_dict('mxm_dataset_train.txt') \n",
    "print(\"Song dict\")\n",
    "\n",
    "# Import mxm 779 to get id to name mapping later\n",
    "\n",
    "# all_song_meta_dict = dict()\n",
    "# with open('../../data/mxm_779k_matches.txt','r') as f:\n",
    "#     lines = f.readlines()\n",
    "#     for i in range(18, len(lines)):\n",
    "#         line = lines[i].split('<SEP>')\n",
    "#         MSDID = line[0]\n",
    "#         artist = line[1]\n",
    "#         title = line[2]\n",
    "#         all_song_meta_dict[str(MSDID)] = {'artist': artist, 'title': title}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CvZ9F4opgC-8",
    "outputId": "10a57211-53cc-4976-9630-0fd4e45e8184"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reverse map\n",
      "Training set\n",
      "Fast text embeddings\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "words = list(word_emb_dict.keys())\n",
    "word2idx = {word: i for i,word in enumerate(words)}\n",
    "\n",
    "K = 39\n",
    "\n",
    "tensor_embeddings = torch.cat([torch.unsqueeze(word_emb_dict[word],0) for word in words], 0) # (M, N)\n",
    "\n",
    "np_embeddings = torch.cat([torch.unsqueeze(word_emb_dict[word],0) for word in words], 0).cpu().numpy()\n",
    "\n",
    "gm = GaussianMixture(n_components=K, random_state=0).fit(np_embeddings)\n",
    "classes = torch.tensor(gm.predict(np_embeddings), device=\"cuda\") # (M,)\n",
    "probs = torch.tensor(gm.predict_proba(np_embeddings), device=\"cuda\") # (M, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAijUkPEEavh"
   },
   "outputs": [],
   "source": [
    "# Failed attempt to manually implement Gaussian Mixture Model\n",
    "\n",
    "\"\"\"\n",
    "from torch.distributions import multivariate_normal\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "# K = 3\n",
    "# epsilon = 1e-8\n",
    "# iters = 40\n",
    "def gmm(data):\n",
    "    # X = np.array(data)\n",
    "    X = data\n",
    "    M = X.shape[0]\n",
    "    N = X.shape[1]\n",
    "    # weights = np.ones((K)) / K\n",
    "    weights = torch.ones((K)) / K\n",
    "    # means = np.random.choice(X.flatten(), (K,X.shape[1]))\n",
    "    perm = torch.randperm(X.numel())\n",
    "    idx = perm[:K*N]\n",
    "    means = torch.reshape(X.flatten()[idx], (K,N))\n",
    "    # cov = np.array([make_spd_matrix(X.shape[1]) for _ in range(K)])\n",
    "    cov = torch.tensor([make_spd_matrix(N) for _ in range(K)], device='cuda').float()\n",
    "    scale_tril = torch.tril(cov)\n",
    "\n",
    "    bayes = []\n",
    "\n",
    "    for step in range(iters):\n",
    "        # likelihood = torch.tensor([multivariate_normal.pdf(x=X, mean=means[j], cov=cov[j]) for j in range(K)])\n",
    "        print(multivariate_normal.MultivariateNormal(means[2], covariance_matrix=cov[2]).log_prob(X))\n",
    "        likelihood = torch.cat([torch.unsqueeze(multivariate_normal.MultivariateNormal(means[j], covariance_matrix=cov[j]).log_prob(X), 0) for j in range(K)], 0)\n",
    "        assert likelihood.shape == (K, M)\n",
    "\n",
    "        bayes = []\n",
    "        for j in range(K):\n",
    "            bayes.append((likelihood[j] * weights[j]) / (torch.sum(torch.cat([torch.unsqueeze(likelihood[i] * weights[i], 0) for i in range(K)], 0), axis=0)+epsilon))\n",
    "\n",
    "            means[j] = torch.sum(torch.reshape(bayes[j], (M, 1)) * X) / (torch.sum(bayes[j]+epsilon))\n",
    "            cov[j] = torch.mm((torch.reshape(bayes[j], (M, 1)) * (X - means[j])).T, (X - means[j])) / (torch.sum(bayes[j])+epsilon)\n",
    "            \n",
    "            weights[j] = torch.mean(bayes[j])\n",
    "\n",
    "            assert cov.shape == (K, N, N)\n",
    "            assert means.shape == (K, N)\n",
    "\n",
    "        \n",
    "    \n",
    "    classes = []\n",
    "    for i in range(M):\n",
    "        # likelihood = torch.tensor([multivariate_normal.pdf(x=X[i,:], mean=means[j], cov=cov[j]) for j in range(K)])\n",
    "        likelihood = torch.tensor([multivariate_normal.MultivariateNormal(means[j], cov[j]).log_prob(X[i,:]) for j in range(K)])\n",
    "        print(i, likelihood)\n",
    "        classes.append(torch.argmax(likelihood)+1)\n",
    "    \n",
    "    return classes, weights, means, cov\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G37XPTXLSJ17"
   },
   "outputs": [],
   "source": [
    "# Determine the optimal number of clusters, evaulated with Akaike information criterion on test set\n",
    "_, _, word_emb_dict_test = get_song_emb_dict('mxm_dataset_test.txt')\n",
    "\n",
    "np_embeddings_test = torch.cat([torch.unsqueeze(word_emb_dict_test[word],0) for word in words], 0).cpu().numpy()\n",
    "\n",
    "\n",
    "aic = []\n",
    "clusters = range(31,35)\n",
    "\n",
    "for K in clusters:\n",
    "  gm = GaussianMixture(n_components=K, random_state=0).fit(np_embeddings)\n",
    "  aic.append(gm.aic(np_embeddings_test))\n",
    "  print(K)\n",
    "\n",
    "print(aic)\n",
    "\n",
    "\"\"\"\n",
    "AIC for train\n",
    "clusters = [10, 30, 50, 80, 100, 150, 200]\n",
    "aic = [-4504852.787373134, -6666515.6722224485, -6703928.966121189, -5264487.663826261, -3913524.8514031116, 126418.23396109417, 4308583.780426782]\n",
    "\n",
    "clusters = [10, 20, 30, 40, 50, 60, 70, 80]\n",
    "aic = [-4504852.787373134, -5597897.493379482, -6666515.6722224485, -6688446.631000172, -6703928.966121189, -6177245.378351711, -5778542.452237887, -5264487.663826261]\n",
    "\n",
    "clusters = [30, 35, 40, 45, 50, 55]\n",
    "aic = [-6666515.6722224485, -6654718.370988596, -6688446.631000172, -6654966.182520773, -6703928.966121189, -6312888.754963251]\n",
    "\n",
    "clusters = range(46, 55)\n",
    "aic = [-6613663.237831932, -6690252.560396293, -6707868.594154408, -6695732.93230935, -6703928.966121189, -6605595.489092547, -6576743.819046922, -6733423.197465405, -6717831.644150782]\n",
    "\n",
    "clusters = range(41,46)\n",
    "aic = [-6735285.285983134, -6691415.717371201, -6650176.874654621, -6664396.813439354, -6654966.182520773]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "AIC for test\n",
    "clusters = [10, 30, 50, 80, 100, 150, 200]\n",
    "aic = [-4483687.5748528335, -6462898.944675984, -6580692.25876198, -5107816.440313572, -3772441.9127039015, 169570.8663827367, 4325081.014762333]\n",
    "\n",
    "clusters = [10, 20, 30, 40, 50, 60, 70, 80]\n",
    "aic = [-4483687.5748528335, -5597856.083688019, -6462898.944675984, -6746603.306217089, -6580692.25876198, -6266922.301159833, -5729930.116943039, -5107816.440313572]\n",
    "\n",
    "clusters = [30, 35, 40, 45, 50, 55]\n",
    "aic = [-6462898.944675984, -6750139.253499374, -6746603.306217089, -6659716.89766887, -6580692.25876198, -6504312.897404702]\n",
    "\n",
    "clusters = range(36, 45)\n",
    "aic = [-6668556.477066301, -6829178.28346259, -6829365.392411279, -6810971.983992221, -6746603.306217089, -6795956.870910021, -6743121.8986961655, -6700375.869984798, -6670593.19445217]\n",
    "\n",
    "clusters = range(31,35)\n",
    "aic = [-6471749.693533244, -6457655.665048353, -6503741.977487061, -6535845.925271023]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXRze_XGpiFJ"
   },
   "outputs": [],
   "source": [
    "# Obtain class matrices from tensors\n",
    "\n",
    "id = ids[25]\n",
    "S = len(songs_dict[id])\n",
    "count = []\n",
    "indices = []\n",
    "for tup in songs_dict[id]:\n",
    "  indices.append(word2idx[tup[0]])\n",
    "  count.append(tup[1])\n",
    "\n",
    "count = torch.tensor(count, device=\"cuda\")\n",
    "indices = torch.tensor(indices, device=\"cuda\")\n",
    "lyrics_to_cluster_probs = torch.index_select(probs, 0, indices)\n",
    "lyrics_to_embs = torch.index_select(tensor_embeddings, 0, indices)\n",
    "class_matrix = torch.mm(torch.transpose(lyrics_to_cluster_probs, 0, 1).double(), (lyrics_to_embs * count.view(-1, 1).double()))\n",
    "\n",
    "# S: # of words in song bag-of-words\n",
    "# count: S x 1 # no of occurences for each word\n",
    "# lyrics_to_cluster_probs: S x K  Produce by filtering the rows of probs down to only the words in the song\n",
    "# lyrics_to_embs: S x N  Produce by filtering rows of tensor_embeddings down to only words in the song\n",
    "# Then the desired K x N class matrix should be = lyrics_to_cluster_probs.T @ (lyrics_to_embs * count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BDqWbFiNleGv"
   },
   "outputs": [],
   "source": [
    "# print(len(tensor_embeddings), tensor_embeddings[0].numel())\n",
    "# numpy_embeddings = tensor_embeddings.cpu().numpy()\n",
    "# print(numpy_embeddings[0:10])\n",
    "\n",
    "# classes, weights, means, cov = gmm(tensor_embeddings)\n",
    "print(\"GMM\")\n",
    "\n",
    "cluster_set = [[] for _ in range(K)]\n",
    "cluster_dict = {}\n",
    "\n",
    "for i,val in enumerate(classes):\n",
    "    cluster_set[val - 1].append(words[i])\n",
    "    cluster_dict[words[i]] = val\n",
    "\n",
    "pp.pprint(cluster_set)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LyricRec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
